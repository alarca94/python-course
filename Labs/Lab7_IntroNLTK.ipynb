{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para Lingüistas\n",
    "\n",
    "Notebook 7: Introducción a NLTK\n",
    "\n",
    "Alejandro Ariza\n",
    "\n",
    "Universitat de Barcelona 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook comenzaremos a trabajar con NLTK.\n",
    "\n",
    "Cargaremos un corpus e intentaremos procesarlo.\n",
    "\n",
    "Aplicaremos transformaciones básicas como segmentación de frases y tokenización.\n",
    "\n",
    "Calcularemos estadísticas básicas usando bucles así como funciones de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar los paquetes importantes para hoy\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar todos los recursos necesarios para hoy\n",
    "from nltk.corpus import reuters\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conseguir todos los identificadores de los ficheros en el corpus reuters\n",
    "print(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzaremos trabajando con un único fichero, 'test/14826'\n",
    "c_fid = 'test/14826'\n",
    "\n",
    "# Ahora extraemos la versión sin procesar de este fichero\n",
    "test_raw = reuters.raw(c_fid)\n",
    "\n",
    "print(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También disponemos de la versión tokenizada del fichero\n",
    "test_tok = reuters.words(c_fid)\n",
    "\n",
    "print(test_tok[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora obtenemos la versión tokenizada y segmentada por frases del fichero\n",
    "test_sent = reuters.sents(c_fid)\n",
    "\n",
    "print(test_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tiene una herramienta que puede automáticamente tokenizar un corpus\n",
    "manual_tok = word_tokenize(test_raw)\n",
    "\n",
    "# Veamos el corpus tokenizado\n",
    "# Compáralo con el corpus test_tok\n",
    "print(manual_tok[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, usemos la función split() para separar las palabras\n",
    "split_tok = test_raw.split()\n",
    "\n",
    "# Veamos y comparemos lo anterior con esta versión también\n",
    "print(split_tok[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK también tiene una herramienta que puede automáticamente separar las frases de un corpus\n",
    "manual_sent = sent_tokenize(test_raw)\n",
    "\n",
    "# Observa las primeras dos frases\n",
    "print(manual_sent[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tiene una herramienta que cuenta la frecuencia de las palabras en un corpus\n",
    "fd = FreqDist(test_tok)\n",
    "print(fd.most_common(10))\n",
    "fd.plot(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tiene una función que calcula los bigramas de una lista de tokens\n",
    "# Observa la siguiente sentencia\n",
    "test_bigr = list(bigrams(test_tok))\n",
    "\n",
    "print(test_bigr[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea 1\n",
    "# \n",
    "# Tokeniza el corpus reuters usando \"split\" y \"word_tokenize\"\n",
    "# Compara el resultado con la versión ya tokenizada del corpus\n",
    "# \n",
    "# Primero, escribe tu código para un único id de fichero ('test/14826')\n",
    "# Después, ejecuta el código en todo el corpus (sin tener que especificar el id de fichero)\n",
    "# \n",
    "# Compara los resultados\n",
    "# Cuenta el número de tokens en cada \"corpus\"\n",
    "# Cuenta el número de tokens únicos en cada \"corpus\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea 2\n",
    "# \n",
    "# Para el corpus creado con word_tokenize, calcula las palabras más frecuentes\n",
    "# \n",
    "#    - contándolas manualmente (al igual que hicimos en las clases previas)\n",
    "#    - usando FreqDist\n",
    "#\n",
    "#    Compara las palabras más frecuentes - ¿son los mismos resultados con las mismas frecuencias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea 3\n",
    "# \n",
    "# Para el corpus creado con word_tokenize y el corpus pre-tokenizado, obtén la lista de bigramas\n",
    "# Calcula y compara la distribución de frecuencias de los bigramas para los dos corpus\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tareas avanzadas\n",
    "# \n",
    "# Tarea 4 - calcula la frecuencia de las palabras en el corpus, ignorando múltiples repeticiones de la palabra en\n",
    "# una frase e.g.: [\"Esto no es aquello, ¡sino esto!\",\"Es muy decepcionante, pero es lo que es\"]\n",
    "#       \n",
    "#       En este corpus:\n",
    "#           la frecuencia de \"esto\" es 1 (1 en frase 1, 0 en frase 2)\n",
    "#           la frecuencia de \"es\" es 2 (1 en frase 1, 1 en frase 2)\n",
    "#       Ignoramos múltiples repeticiones de la misma palabra en la misma frase\n",
    "#\n",
    "#       En esta tarea puedes usar el corpus existente que ya está separado por frases y palabras\n",
    "#       o usar las funciones sent_tokenize y word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tareas avanzadas\n",
    "# \n",
    "# Tarea 5 - similar a la tarea 4, calcula la frecuencia de los bigramas ignorando múltiples repeticiones\n",
    "# dentro de la misma frase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
