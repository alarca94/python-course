{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para Lingüistas\n",
    "\n",
    "Notebook 10: Análisis de sentimiento\n",
    "\n",
    "Alejandro Ariza\n",
    "\n",
    "Universitat de Barcelona 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook, veremos como hacer analizar el sentimiento de los textos de forma sencilla utilizando Naïve Bayes.\n",
    "\n",
    "En concreto, los objetivos de esta práctica serán los siguientes:\n",
    "* Entrenar un modelo Naive Bayes en una tarea de sentiment analysis\n",
    "* Evaluar vuestro modelo\n",
    "* Calcular ratios de palabras positivas a palabras negativas\n",
    "* Hacer un análisis de error\n",
    "* Predecir en vuestros propios tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que necesiteis descargar los siguientes paquetes de NLTK si todavía no los teneis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los tweets positivos y negativos que vamos a usar\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(f'Tenemos {len(all_positive_tweets)} tweets positivos y {len(all_negative_tweets)} tweets negativos')\n",
    "print(f'Ejemplo de tweet con sentimiento positivo: {all_positive_tweets[0]}')\n",
    "print(f'Ejemplo de tweet con sentimiento negativo: {all_negative_tweets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en training y testing. Utilizaremos 80% de los datos para training y 20% para testing.\n",
    "# Evitamos asumir que sabemos que el número de muestras positivas y negativases el mismo\n",
    "n_pos_train = int(len(all_positive_tweets) * 0.8)\n",
    "n_neg_train = int(len(all_negative_tweets) * 0.8)\n",
    "\n",
    "test_pos = all_positive_tweets[n_pos_train:]\n",
    "train_pos = all_positive_tweets[:n_pos_train]\n",
    "test_neg = all_negative_tweets[n_neg_train:]\n",
    "train_neg = all_negative_tweets[:n_neg_train]\n",
    "\n",
    "# Juntamos las muestras positivas y negativas tanto del train set como del test set\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# Obtenemos el target a clasificar -- el sentimiento del tweet será 0 (negativo) o 1 (positivo)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Procesado de los datos\n",
    "\n",
    "Como ya sabemos, para un modelo de Machine Learning es necesario transformar los datos incluyendo pasos como:\n",
    "- **Borrar el ruido**: Palabras que no añaden mucha información a la hora de predecir el sentimiento.\n",
    "- Borrar peculiaridades de los tweets como símbolos de RTs, hyperlinks, hashtags, etc que no aportan mucho a la hora de predecir el sentimiento (positivo o negativo) del tweet.\n",
    "- Tokenización por palabras.\n",
    "- Borrar los signos de puntuación del tweet.\n",
    "- Pasar a minúsculas.\n",
    "- Finalmente, utilizaremos stemming para evitar tratar con diferentes variaciones de cada palabra. De esta forma, trataremos términos como \"motivation\", \"motivated\", y \"motivate\" de forma similar usando el stem \"motiv-\".\n",
    "\n",
    "Lo primero que haremos será implementar la función `process_tweet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    # Lo primero que vamos a hacer es borrar las URL con la librería \"re\" que hemos importado al principio\n",
    "    # Debéis utilizar el método re.sub() con el tweet (Rellenad los argumentos de entrada)\n",
    "    url_regex = '([…\\-—]\\s)?(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))' \\\n",
    "                '([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'\n",
    "    tweet = re.sub(None, None, None)\n",
    "    \n",
    "    # Borramos las menciones a otros usuarios\n",
    "    user_regex = '@[\\w]+'\n",
    "    tweet = re.sub(None, None, None)\n",
    "    \n",
    "    # Borramos los símbolos de hashtag y RT\n",
    "    symbol_regex = '(RT |#)'\n",
    "    tweet = re.sub(None, None, None)\n",
    "    \n",
    "    # Transformamos a minúsculas el tweet\n",
    "    tweet = None\n",
    "    \n",
    "    # Para tokenizar el tweet vamos a utilizar la clase TweetTokenizer en vez de la función word_tokenize()\n",
    "    tt = TweetTokenizer()\n",
    "    tweet = None\n",
    "    \n",
    "    # Borramos símbolos de puntuación sueltos que queden en la tokenización\n",
    "    invalid_symbols = '@?^=%&/~+#-[]()...!\\\\:;,_*’…><'\n",
    "    tweet = None\n",
    "    \n",
    "    # Borramos las stopwords\n",
    "    tweet = None\n",
    "    \n",
    "    # Realizamos el stemming utilizando la clase que hemos importado al principio PorterStemmer\n",
    "    ps = PorterStemmer()\n",
    "    tweet = None\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(process_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado:** ['hello', 'great', 'day', ':)', 'good', 'morn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Implementar vuestras funciones auxiliares\n",
    "\n",
    "Para ayudar a vuestro modelo Naive Bayes, necesitaréis construir un diccionario donde las claves serán tuplas (palabra, etiqueta) y los valores su frecuencia correspondiente en el corpus. Las etiquetas recordad que serán 0 (sentimiento negativo) o 1 (sentimiento positivo). Esta función se llamará count_tweets().\n",
    "\n",
    "También implementaréis una función auxiliar llamada lookup() que tomará el diccionario de frecuencias, una palabra y una etiqueta y devolverá el número de veces que esa palabra aparece con esa etiqueta en el corpus de tweets.\n",
    "\n",
    "- Cuando construyáis el diccionario de frecuencias, se les asignará a todas las palabras de un tweet la misma etiqueta, es decir, la etiqueta del tweet al que pertenecen.\n",
    "- Recordad procesar el tweet antes de añadir los tokens al diccionario.\n",
    "- La función count_tweets() recibirá una lista de tweets, una lista de etiquetas correspondiente a esos tweets y el diccionario que contendrá las frecuencias resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: diccionario donde guardaremos las tuplas (palabra, etiqueta) y sus frecuencias\n",
    "        tweets: una lista de tweets\n",
    "        ys: una lista de etiquetas correspondientes a los tweets (contiene 0s y 1s)\n",
    "    Output:\n",
    "        result: el diccionario que hemos rellenado\n",
    "    '''\n",
    "\n",
    "    ### Reemplaza donde pone None con vuestro código ###\n",
    "    for y, tweet in None:\n",
    "        for word in None:\n",
    "            # Define la clave actual en forma de tupla (palabra, etiqueta)\n",
    "            pair = None\n",
    "\n",
    "            # si la clave existe, incrementa la frecuencia\n",
    "            if pair in result:\n",
    "                result[pair] += None\n",
    "\n",
    "            # sino, la clave es nueva, anádela al diccionario y ponle frecuencia 1\n",
    "            else:\n",
    "                result[pair] = None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúa tu función\n",
    "\n",
    "\n",
    "result = {}\n",
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Entrenar el modelo usando Naive Bayes\n",
    "\n",
    "Naive bayes es un algoritmo que puede ser usado para realizar análisis de sentimiento. Requiere de poco tiempo para entrenar al igual que predecir nuevas muestras.\n",
    "\n",
    "#### Los pasos para entrenar un clasificador Naive Bayes:\n",
    "- Identificar el número de clases que tenemos.\n",
    "- Darle una probabilidad a cada clase.\n",
    "\n",
    "$P(D_{pos})$ es la probabilidad de que el documento es positivo.\n",
    "$P(D_{neg})$ es la probabilidad de que el documento es negativo.\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Donde $D$ es el número total de documentos (tweets en nuestro caso), $D_{pos}$ es el número total de tweets positivos y $D_{neg}$ es el número total de tweets negativos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilidades a priori (Prior)\n",
    "\n",
    "La probabilidad a priori representa la probabilidad subyacente en la población objetivo de que un tuit sea positivo frente a negativo. En otras palabras, si no tuviéramos información específica y seleccionamos a ciegas un tweet del conjunto de población, ¿cuál es la probabilidad de que sea positivo frente a que sea negativo?\n",
    "\n",
    "Matemáticamente, este ratio sería: $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "A veces, para evitar perder precisión con las probabilidades, nos interesa más trabajar con los logaritmos de las probabilidades.\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Recordad que $log(\\frac{A}{B})$ es lo mismo que $log(A) - log(B)$, por lo que la primera ecuación puede ser reformulada de la siguiente forma:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilidad de que una palabra sea positiva o negativa\n",
    "Para calcular la probabilidad positiva y la probabilidad negativa de una palabra específica en el vocabulario, usaremos las siguientes entradas:\n",
    "\n",
    "- $freq_{pos}$ y $freq_{neg}$ son las frecuencias de esa palabra específica en la clase positiva o negativa. En otras palabras, la frecuencia positiva de una palabra es el número de veces que la palabra aparece con la etiqueta de 1.\n",
    "- $N_{pos}$ y $N_{neg}$ son el número total de palabras positivas y negativas para todos los documentos (para todos los tweets), respectivamente.\n",
    "- $V$ es el número de palabras únicas en todo el conjunto de documentos, para todas las clases, ya sean positivas o negativas.\n",
    "\n",
    "Usaremos estos valores para calcular la probabilidad positiva y negativa de una palabra específica usando esta fórmula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Observad que agregamos el \"+1\" en el numerador para el suavizado aditivo.  Este [artículo de la Wikipedia](https://en.wikipedia.org/wiki/Additive_smoothing) explica en qué consiste el suavizado aditivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "Para calcular el loglikelihood de la misma palabra, podemos implementar la siguiente ecuación:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crear el diccionario `freqs`\n",
    "- Dada vuestra función `count_tweets()`, podéis generar un diccionario llamado `freqs` que contenga todas las frecuencias.\n",
    "- En este diccionario `freqs`, cada clave es una tupla (palabra, etiqueta)\n",
    "- El valor es el número de veces que ha aparecido.\n",
    "\n",
    "Usaremos este diccionario en varias partes de lo que queda de práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construye el diccionario de frecuencias para su futuro uso\n",
    "\n",
    "freqs = count_tweets({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instrucciones\n",
    "Dado un diccionario de frecuencias, `train_x` (una lista de tweets) y `train_y` (una lista de etiquetas para cada tweet), implementad un clasificador naive bayes.\n",
    "\n",
    "##### Calculad $V$\n",
    "- Número de palabras únicas que aparece en el diccionario de frecuencias.\n",
    "\n",
    "##### Calculad $freq_{pos}$ y $freq_{neg}$\n",
    "- Usando `freqs`, calculad la frecuencia positiva y negativa de cada palabra.\n",
    "\n",
    "##### Calculad $N_{pos}$ y $N_{neg}$\n",
    "- Usando `freqs`, calculad el número total de palabras positivas y negativas.\n",
    "\n",
    "##### Calculad $D$, $D_{pos}$, $D_{neg}$\n",
    "- Usando `train_y`, calculad el número de documentos (tweets) $D$, así como documentos positivo $D_{pos}$ y negativos $D_{neg}$.\n",
    "- Calculad la probabilidad de que un documento sea positivo $P(D_{pos})$, y negativo $P(D_{neg})$\n",
    "\n",
    "##### Calculad el logprior de ambos conjuntos, positivos y negativos\n",
    "- $logprior = log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculad el log likelihood\n",
    "- Iterad por cada palabra del vocabulario, usad vuestra función `lookup` para conseguir las frecuencias positivas, $freq_{pos}$, y negativas, $freq_{neg}$, para la palabra especificada.\n",
    "- Calcular la probabiliad positiva de cada palabra $P(W_{pos})$, y la negativa $P(W_{neg})$ usando las ecuaciones 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Nota:** Usaremos un diccionario para almacenar las probabilidades logarítmicas de cada palabra. La clave es la palabra, el valor es la probabilidad logarítmica de esa palabra).\n",
    "\n",
    "- Entonces, podéis calcular el loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: diccionario con claves (word, label) y valores - frecuencias\n",
    "        train_x: lista de tweets\n",
    "        train_y: lista de etiquetas (0,1)\n",
    "    Output:\n",
    "        logprior: priori. (ecuación 3)\n",
    "        loglikelihood: log likelihood (equación 6)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### Reemplaza donde pone None con vuestro código ###\n",
    "\n",
    "    # calcula V, número de palabras únicas en el vocabulario\n",
    "    vocab = None\n",
    "    V = None\n",
    "\n",
    "    # calcula N_pos y N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # si la etiqueta es positiva (mayor que cero)\n",
    "        if None > 0:\n",
    "\n",
    "            # Incrementa la frecuencia positiva para este par (palabra, etiqueta)\n",
    "            N_pos += None\n",
    "\n",
    "        # sino, la etiqueta es negativa\n",
    "        else:\n",
    "\n",
    "            # incrementa la frecuancia negativa\n",
    "            N_neg += None\n",
    "\n",
    "    # Calcula D, el número de documentos\n",
    "    D = None\n",
    "\n",
    "    # Calcula D_pos, el número de documentos positivos (puedes utilizar la función sum())\n",
    "    D_pos = None\n",
    "\n",
    "    # Calcula D_neg, el número de documentos negativos (a partir de D y D_pos)\n",
    "    D_neg = None\n",
    "\n",
    "    # Calcula el logprior\n",
    "    logprior = None\n",
    "\n",
    "    # Para cada palabra del vocabulario...\n",
    "    for word in vocab:\n",
    "        # recupera la frecuencia positiva y negativa de la palabra\n",
    "        freq_pos = None\n",
    "        freq_neg = None\n",
    "\n",
    "        # calcula la probabilidad de que sea positiva, y negativa\n",
    "        p_w_pos = None\n",
    "        p_w_neg = None\n",
    "\n",
    "        # calcula el log likelihood de la palabra\n",
    "        loglikelihood[word] = None\n",
    "\n",
    "    return logprior, loglikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**:\n",
    "\n",
    "0.0\n",
    "\n",
    "9218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Evalúa tu clasificador\n",
    "\n",
    "Ahora puedes evaluar tu clasificador Naive Bayes prediciendo el sentimiento de muestras nuevas.\n",
    "\n",
    "#### Implementa `naive_bayes_predict`\n",
    "**Instrucciones**:\n",
    "Implementa la función `naive_bayes_predict` para hacer predicciones de tweets.\n",
    "* La función recibe como argumentos `tweet`, `logprior`, `loglikelihood`.\n",
    "* Devuelve la probabilidad de que el tweet pertenezca a la clase positiva o negativa.\n",
    "* Para cada tweet, suma los loglikelihoods de cada palabra que aparece en el tweet.\n",
    "* También añade el logprior a la suma para conseguir la predicción de sentimiento del tweet.\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Nota\n",
    "El prior lo obtenemos de los datos de training que hemos obtenido con una partición equilibrada de tweets positivos y negativos (4000 de cada). Esto significa que el ratio de positivo a negativo es 1 y, por tanto, el logprior es 0.\n",
    "\n",
    "De todas formas, aunque su valor sea 0.0, acordaros de añadir el logprior, porque cuando los datos no estén perfectamente balanceados, su valor cambiará y afectará a los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: un string\n",
    "        logprior: un número\n",
    "        loglikelihood: un diccionario de words y probabilidades\n",
    "    Output:\n",
    "        p: suma de loglikelihoods de cada palabra en el tweet (si se encuentra en el diccionario) + logprior\n",
    "\n",
    "    '''\n",
    "    ### Reemplaza donde pone None con vuestro código ###\n",
    "    # procesad el tweet para obtener una lista de tokens\n",
    "    word_l = None\n",
    "\n",
    "    # inicializad la probabilidad a cero\n",
    "    p = None\n",
    "\n",
    "    # añadid el logprior\n",
    "    p += None\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # comprobad si la palabra existe en el diccionario de loglikelihoods\n",
    "        if word in None:\n",
    "            # añadid el log likelihood de la palabra a la probabilidad total\n",
    "            p += None\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenta con tu propio tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**:\n",
    "- The expected output is around 1.56\n",
    "- The sentiment is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementa test_naive_bayes\n",
    "**Instrucciones**:\n",
    "* Implementa `test_naive_bayes` para comprobar la precisión de vuestras predicciones.\n",
    "* La función recibe `test_x`, `test_y`, log_prior, y loglikelihood\n",
    "* Devuelve la precisión de vuestro modelo.\n",
    "* Primero, usa la función `naive_bayes_predict` para hacer la predicción de cada tweet en text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: una lista de tweets\n",
    "        test_y: las etiquetas correspondientes a los tweets\n",
    "        logprior: el logprior\n",
    "        loglikelihood: un diccionario con las loglikelihoods de cada word\n",
    "    Output:\n",
    "        accuracy: (# de tweets clasificados correctamente)/(# total de tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "\n",
    "    ### Reemplaza donde pone None con vuestro código ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # si la predicción es > 0\n",
    "        if None > 0:\n",
    "            # la predicción será 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # sino la predicción será 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # añade la clase predicha a la lista y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # el error es la media de los valores absolutos de las diferencias entre y_hats y test_y\n",
    "    error = None\n",
    "\n",
    "    # la precisión es 1 menos el error\n",
    "    accuracy = None\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Accuracy**:\n",
    "\n",
    "0.9955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta este código para evaluar vuestra función\n",
    "example_tweets = ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great',\n",
    "                  'great great great', 'great great great great']\n",
    "for tweet in example_tweets:\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- I am happy -> 2.14\n",
    "- I am bad -> -1.31\n",
    "- this movie should have been great. -> 2.11\n",
    "- great -> 2.13\n",
    "- great great -> 4.25\n",
    "- great great great -> 6.38\n",
    "- great great great great -> 8.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Análisis de error\n",
    "\n",
    "En esta parte veréis algunos tweets en los que vuestro modelo se ha equivocado. ¿Por qué pensáis que se ha equivocado? ¿El modelo naive bayes ha asumido algo que no debería?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5: Predecir el sentimiento de vuestro propio tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza el texto del tweet por lo que tú quieras\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "schema_names": [
    "NLPC1-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
