{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python para Lingüistas\n",
    "\n",
    "Notebook 8: POS Tagging\n",
    "\n",
    "Alejandro Ariza\n",
    "\n",
    "Universitat de Barcelona 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook veremos como realizar etiquetado POS usando NLTK.\n",
    "\n",
    "Usaremos diferentes etiquetadores y veremos la importancia de seleccionar un buen corpus de entrenamiento.\n",
    "\n",
    "También aprenderemos más acerca de funciones y métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\infr-usu-\n",
      "[nltk_data]     fil.aules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\infr-usu-\n",
      "[nltk_data]     fil.aules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar los paquetes importantes para hoy\n",
    "nltk.download(\"brown\")\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar todos los recursos necesarios para hoy\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Importar numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importar codecs\n",
    "import codecs\n",
    "\n",
    "# Importar etiquetadores\n",
    "from nltk import DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk import ClassifierBasedPOSTagger\n",
    "\n",
    "# Importar el corpus brown\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones avanzadas\n",
    "\n",
    "# Antes de continuar con el etiquetado POS, observermos el comportamiento de una función muy simple\n",
    "\n",
    "var_1 = \"Boo\"\n",
    "var_2 = \"Hoo\"\n",
    "var_3 = \"DooDoo\"\n",
    "\n",
    "def my_simple_function(atr_1, atr_2, atr_3=\"ignórame\"):\n",
    "    \n",
    "    print(\"Mi atr_1 es:\" + str(atr_1))\n",
    "    print(\"Mi atr_2 es:\" + str(atr_2))\n",
    "    print(\"Mi atr_3 es:\" + str(atr_3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi atr_1 es:Boo\n",
      "Mi atr_2 es:Hoo\n",
      "Mi atr_3 es:DooDoo\n"
     ]
    }
   ],
   "source": [
    "# Veamos lo que hace\n",
    "my_simple_function(var_1, var_2, var_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi atr_1 es:DooDoo\n",
      "Mi atr_2 es:Boo\n",
      "Mi atr_3 es:Hoo\n"
     ]
    }
   ],
   "source": [
    "# ¿Qué pasa si cambiamos el orden?\n",
    "# Como podéis ver, la función asigna valores a las variables de entrada basándose en su posición\n",
    "my_simple_function(var_3, var_1, var_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi atr_1 es:Boo\n",
      "Mi atr_2 es:Hoo\n",
      "Mi atr_3 es:ignórame\n"
     ]
    }
   ],
   "source": [
    "# ¿Qué pasa si no añadimos valor para el tercer argumento de entrada?\n",
    "my_simple_function(var_1, var_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_simple_function() missing 1 required positional argument: 'atr_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-593fa59532d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ¿Qué pasa si no añadimos valor para el segundo argumento?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_simple_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: my_simple_function() missing 1 required positional argument: 'atr_2'"
     ]
    }
   ],
   "source": [
    "# ¿Qué pasa si no añadimos valor para el segundo argumento?\n",
    "my_simple_function(var_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mi atr_1 es:Hoo\n",
      "Mi atr_2 es:DooDoo\n",
      "Mi atr_3 es:Boo\n"
     ]
    }
   ],
   "source": [
    "# ¿Qué pasa si le decimos explícitamente qué es cada cosa?\n",
    "my_simple_function(atr_3=var_1, atr_1=var_2, atr_2=var_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiquetado POS en Python\n",
    "\n",
    "En esta clase, veremos diferentes formas de etiquetar automáticamente un corpus con su POS\n",
    "\n",
    "Nos centraremos en los etiquetadores basados en n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtened una lista de categorías del corpus brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conseguid la versión tokenizada y etiquetada de la categoría \"news\"\n",
    "brown_twords = brown.tagged_words(categories='news')\n",
    "\n",
    "# Conseguid la versión que, además, tiene la segmentación de las frases\n",
    "brown_tsents = brown.tagged_sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Las primeras 5 palabras en la versión tokenizada y etiquetada son:\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL')]\n"
     ]
    }
   ],
   "source": [
    "# Imprimid las 5 primeras palabras\n",
    "print(\"\\nLas primeras 5 palabras en la versión tokenizada y etiquetada son:\")\n",
    "print(brown_twords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Las primeras 2 frases en la versión con frases segmentadas son:\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Imprimid las dos primeras frases\n",
    "print(\"\\nLas primeras 2 frases en la versión con frases segmentadas son:\")\n",
    "print(brown_tsents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El conjunto de etiquetas en el corpus brown es:\n",
      "{'BEM', 'NN$-TL', 'PP$-TL', 'PP$', 'NNS$', 'BEDZ', 'DTI-HL', 'RB+BEZ', 'RBR', 'DTX', 'JJT-HL', 'MD-TL', ',-HL', 'BEZ', 'NNS', 'DT-HL', 'BE', 'JJR', 'FW-IN+NN', 'FW-IN+NN-TL', 'JJS', 'VB-HL', 'RB$', 'AP-HL', 'CS', 'DT', 'EX+BEZ', 'NR-HL', 'NN-TL-HL', ')', 'JJ', 'BEDZ*', '.-HL', 'FW-NNS', 'VB+PPO', 'AP$', 'NPS-HL', 'BEDZ-HL', 'JJ-NC', 'ABN', 'NR$-TL', 'JJS-TL', 'CD-TL', 'NP-HL', 'VBZ-HL', 'QL-TL', 'DO', 'NN', 'FW-IN-TL', 'NNS-HL', 'UH-TL', 'OD-TL', 'NPS', 'NNS-TL-HL', 'WDT', 'NN$-HL', 'BER-TL', 'TO-HL', 'PPSS+HVD', 'PP$$', 'NR', 'PPO', 'NNS-TL', 'PPS+BEZ-HL', 'BED*', 'WP$', 'NP+BEZ', 'RB-TL', 'NPS$', 'VB-TL', 'NP', 'FW-NN', '*', 'CS-HL', 'VBZ', ',', 'TO-TL', 'RP', 'PPSS+HV', 'OD', 'NNS$-TL', '``', 'AT-TL', 'FW-JJ', 'ABX', \"''\", 'WPS', 'FW-AT-HL', 'FW-AT-TL', 'RB-HL', 'PN+HVZ', 'MD', 'DO-HL', 'HVD', 'JJR-TL', 'AT-HL', 'DOD', \"'\", 'BE-HL', 'FW-CC', 'WQL', 'IN-TL', 'PN$', 'HV', 'FW-VB-NC', 'CC', 'FW-AT', 'AP-TL', 'PPSS-HL', 'VBD-HL', 'MD*-HL', 'PPS', 'QLP', 'DTI', 'FW-IN+AT-TL', 'HVD*', 'JJR-HL', 'JJ-HL', 'PPSS', 'MD*', 'VBG-TL', 'ABN-HL', 'HVZ*', 'VB', 'NN-NC', 'BED', 'VBD', 'HVN', 'NP$-TL', 'NN-HL', 'DTS', '--', 'DOZ', 'IN-HL', 'PPSS+BER', 'BER', 'JJT', 'NP$', 'JJ-TL', 'CC-TL', 'CD-HL', 'PPLS', 'VBD-TL', 'CD$', ':-HL', 'BEG', 'PPSS+MD', 'MD-HL', 'DOZ*', 'VBG-HL', 'HVD-HL', 'MD+HV', 'PPS+BEZ', 'WRB', '(', 'NN-TL', ':', 'DO*', 'OD-HL', 'DOD*', 'NR$', 'RB', 'VBN-TL', 'WDT+BEZ', 'HVZ', 'BER*', 'RBT', '*-HL', 'FW-DT', 'NN$', 'IN', 'AP', '(-HL', 'NP-TL-HL', 'BEZ-HL', 'NPS-TL', 'VBN-HL', 'UH', 'FW-IN', 'HVG', 'CC-HL', 'PPS+HVZ', 'WPO', 'FW-JJ-TL', 'FW-WDT', 'EX', 'FW-*', 'VBG', 'PN-HL', 'DT+BEZ', 'BEN', 'FW-PP$-NC', '.', 'FW-VB', 'VBN', 'BER-HL', 'ABL', 'FW-CD', 'AT', 'WPS+BEZ', 'NPS$-TL', 'DT$', 'JJR-NC', 'QL', 'TO', 'PPL', 'FW-NN-TL', 'NP-TL', ')-HL', 'PPSS+BEM', 'CD', 'BEZ*', 'PPS+MD', 'NNS$-HL', 'NR-TL', 'PN', 'RP-HL', 'VBN-TL-HL'}\n"
     ]
    }
   ],
   "source": [
    "# Conseguid el conjunto de todas las etiquetas que aparecen en el corpus brown\n",
    "brown_tags = set([tag for (token,tag) in brown_twords])\n",
    "print(\"\\nEl conjunto de etiquetas en el corpus brown es:\")\n",
    "print(brown_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "El conjunto de etiquetas universales es:\n",
      "{'NOUN', 'ADP', '.', 'X', 'DET', 'PRT', 'NUM', 'PRON', 'ADV', 'CONJ', 'VERB', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "# Conseguid el conjunto de todas las etiquetas que existen en la versión \"universal\"\n",
    "brown_utwords = brown.tagged_words(categories='news',tagset='universal')\n",
    "universal_tags = set([tag for (token,tag) in brown_utwords])\n",
    "print(\"\\nEl conjunto de etiquetas universales es:\")\n",
    "print(universal_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenaremos y evaluaremos diferentes etiquetadores:\n",
    "\n",
    "- default\n",
    "- affix\n",
    "- unigram\n",
    "- bigram\n",
    "- trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n"
     ]
    }
   ],
   "source": [
    "# Etiquetador por defecto\n",
    "\n",
    "# Obtened la versión segmentada y tokenizada de \"news\"\n",
    "# Esta es la versión no etiquetada que vamos a etiquetar\n",
    "brown_sents = brown.sents(categories='news')\n",
    "\n",
    "# Obtened una lista de todas las etiquetas en el corpus\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "\n",
    "# Obtened la etiqueta más frecuente en el corpus\n",
    "most_frequent_tag = nltk.FreqDist(tags).max()\n",
    "\n",
    "# Imprimid la etiqueta más frecuente:\n",
    "print(most_frequent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase etiquetada con el etiquetador por defecto:\n",
      "[('the', 'NN'), ('quick', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Configurad un etiquetador por defecto\n",
    "# El etiquetador por defecto asigna la misma etiqueta por defecto a todos los tokens en el corpus\n",
    "# Lo configuraremos para que anote la etiqueta más frecuente\n",
    "default_tagger = nltk.DefaultTagger(most_frequent_tag)\n",
    "\n",
    "my_sent = \"the quick brown fox jumped over the lazy dog\".split()\n",
    "print(\"La frase etiquetada con el etiquetador por defecto:\")\n",
    "print(default_tagger.tag(my_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del etiquetador por defecto en el corpus brown es:\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "# Evaluad el etiquetador por defecto en el corpus:\n",
    "print(\"La precisión del etiquetador por defecto en el corpus brown es:\")\n",
    "print(round(default_tagger.evaluate(brown_tsents),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el resto de etiquetadores, dividiremos el corpus en train y test:\n",
    "test_corpus = brown_tsents[:1000]\n",
    "train_corpus = brown_tsents[1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador affix es:\n",
      "[('The', None), ('Fulton', 'NP'), ('County', 'NN'), ('Grand', 'NP'), ('Jury', None), ('said', None), ('Friday', 'NR'), ('an', None), ('investigation', 'NN'), ('of', None), (\"Atlanta's\", 'NP$'), ('recent', 'NN'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBN'), ('``', None), ('no', None), ('evidence', 'NN'), (\"''\", None), ('that', None), ('any', None), ('irregularities', 'NNS'), ('took', None), ('place', 'NN'), ('.', None)]\n",
      "\n",
      "La precisión del etiquetador affix en el corpus es:\n",
      "0.26\n"
     ]
    }
   ],
   "source": [
    "# Entrena el etiquetador affix\n",
    "affix_tagger = AffixTagger(train_corpus)\n",
    "\n",
    "# Etiqueta el corpus\n",
    "affix_sents = affix_tagger.tag_sents(brown_sents)\n",
    "\n",
    "# Imprime la primera frase y la precisión\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador affix es:\")\n",
    "print(affix_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador affix en el corpus es:\")\n",
    "print(round(affix_tagger.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador basado en unigramas es:\n",
      "[('The', 'AT'), ('Fulton', None), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', None), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "La precisión del etiquetador basado en unigramas en el corpus es:\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "unigram_tagger = UnigramTagger(train_corpus) \n",
    "\n",
    "# Etiquetado\n",
    "uni_sents = unigram_tagger.tag_sents(brown_sents)\n",
    "\n",
    "# Visualización de resultados\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador basado en unigramas es:\")\n",
    "print(uni_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador basado en unigramas en el corpus es:\")\n",
    "print(round(unigram_tagger.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador basado en bigramas es:\n",
      "[('The', 'AT'), ('Fulton', None), ('County', None), ('Grand', None), ('Jury', None), ('said', None), ('Friday', None), ('an', None), ('investigation', None), ('of', None), (\"Atlanta's\", None), ('recent', None), ('primary', None), ('election', None), ('produced', None), ('``', None), ('no', None), ('evidence', None), (\"''\", None), ('that', None), ('any', None), ('irregularities', None), ('took', None), ('place', None), ('.', None)]\n",
      "\n",
      "La precisión del etiquetador basado en bigramas en el corpus es:\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "bigram_tagger = BigramTagger(train_corpus)\n",
    "\n",
    "# Etiquetado\n",
    "bi_sents = bigram_tagger.tag_sents(brown_sents)\n",
    "\n",
    "# Visualización de resultados\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador basado en bigramas es:\")\n",
    "print(bi_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador basado en bigramas en el corpus es:\")\n",
    "print(round(bigram_tagger.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador basado en trigramas es:\n",
      "[('The', 'AT'), ('Fulton', None), ('County', None), ('Grand', None), ('Jury', None), ('said', None), ('Friday', None), ('an', None), ('investigation', None), ('of', None), (\"Atlanta's\", None), ('recent', None), ('primary', None), ('election', None), ('produced', None), ('``', None), ('no', None), ('evidence', None), (\"''\", None), ('that', None), ('any', None), ('irregularities', None), ('took', None), ('place', None), ('.', None)]\n",
      "\n",
      "La precisión del etiquetador basado en trigramas en el corpus es:\n",
      "0.06\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "trigram_tagger = TrigramTagger(train_corpus)\n",
    "\n",
    "# Etiquetado\n",
    "tri_sents = trigram_tagger.tag_sents(brown_sents)\n",
    "\n",
    "tri_sents = trigram_tagger.tag_sents(brown_sents)\n",
    "\n",
    "# Visualización de resultados\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador basado en trigramas es:\")\n",
    "print(tri_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador basado en trigramas en el corpus es:\")\n",
    "print(round(trigram_tagger.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador basado en bigramas con backoff es:\n",
      "[('The', 'AT'), ('Fulton', None), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'WPS'), ('any', 'DTI'), ('irregularities', None), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "La precisión del etiquetador basado en bigramas con backoff en el corpus es:\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "bigram_tagger_backoff = BigramTagger(train_corpus,backoff=unigram_tagger)\n",
    "\n",
    "# Etiquetado\n",
    "bi_sents_bo = bigram_tagger_backoff.tag_sents(brown_sents)\n",
    "\n",
    "\n",
    "# Visualización de resultados\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador basado en bigramas con backoff es:\")\n",
    "print(bi_sents_bo[0])\n",
    "print(\"\\nLa precisión del etiquetador basado en bigramas con backoff en el corpus es:\")\n",
    "print(round(bigram_tagger_backoff.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "# Tarea 1\n",
    "# Entrenar y evaluar en el mismo corpus en vez de usar particiones separadas\n",
    "# ¿Cuál es el resultado?\n",
    "\n",
    "print(round(bigram_tagger_backoff.evaluate(train_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador basado en bigramas con backoff es:\n",
      "[('The', 'AT'), ('Fulton', 'NP'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'WPS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "La precisión del etiquetador basado en bigramas con backoff en el corpus es:\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "# Tarea 2\n",
    "# Crear una secuencia de etiquetadores a modo backoff:\n",
    "# trigram -> bigram -> unigram -> affix -> default\n",
    "# Evaluar el etiquetador resultante\n",
    "default_tagger = nltk.DefaultTagger(most_frequent_tag)\n",
    "affix_tagger = AffixTagger(train_corpus, backoff=default_tagger)\n",
    "unigram_tagger = UnigramTagger(train_corpus, backoff=affix_tagger)\n",
    "bigram_tagger = BigramTagger(train_corpus,backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(train_corpus, backoff=bigram_tagger)\n",
    "\n",
    "tri_sents_bo = trigram_tagger.tag_sents(brown_sents)\n",
    "\n",
    "\n",
    "# Visualización de resultados\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador basado en bigramas con backoff es:\")\n",
    "print(tri_sents_bo[0])\n",
    "print(\"\\nLa precisión del etiquetador basado en bigramas con backoff en el corpus es:\")\n",
    "print(round(trigram_tagger.evaluate(test_corpus),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador affix es:\n",
      "[(('The', 'AT'), None), (('Fulton', 'NP-TL'), None), (('County', 'NN-TL'), None), (('Grand', 'JJ-TL'), None), (('Jury', 'NN-TL'), None), (('said', 'VBD'), None), (('Friday', 'NR'), None), (('an', 'AT'), None), (('investigation', 'NN'), None), (('of', 'IN'), None), ((\"Atlanta's\", 'NP$'), None), (('recent', 'JJ'), None), (('primary', 'NN'), None), (('election', 'NN'), None), (('produced', 'VBD'), None), (('``', '``'), None), (('no', 'AT'), None), (('evidence', 'NN'), None), ((\"''\", \"''\"), None), (('that', 'CS'), None), (('any', 'DTI'), None), (('irregularities', 'NNS'), None), (('took', 'VBD'), None), (('place', 'NN'), None), (('.', '.'), None)]\n",
      "\n",
      "La precisión del etiquetador affix en el corpus es:\n",
      "0.19\n"
     ]
    }
   ],
   "source": [
    "# Tarea 3\n",
    "# Intentar cross-domain tagging:\n",
    "# Entrenar usando el corpus \"news\" y evaluar en \"science_fiction\"\n",
    "# Comparar los resultados con un corpus entrenado y evaluado en el mismo dominio (science_fiction)\n",
    "news_sents = brown.tagged_sents(categories='news')\n",
    "science_fiction_sents = brown.tagged_sents(categories='science_fiction')\n",
    "\n",
    "# Entrena el etiquetador affix\n",
    "affix_tagger = AffixTagger(news_sents)\n",
    "\n",
    "# Etiqueta el corpus\n",
    "affix_sents = affix_tagger.tag_sents(news_sents)\n",
    "\n",
    "# Imprime la primera frase y la precisión\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador affix es:\")\n",
    "print(affix_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador affix en el corpus es:\")\n",
    "print(round(affix_tagger.evaluate(science_fiction_sents),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La primera frase etiquetada con el etiquetador affix es:\n",
      "[(('The', 'DET'), None), (('Fulton', 'NOUN'), None), (('County', 'NOUN'), None), (('Grand', 'ADJ'), None), (('Jury', 'NOUN'), None), (('said', 'VERB'), None), (('Friday', 'NOUN'), None), (('an', 'DET'), None), (('investigation', 'NOUN'), None), (('of', 'ADP'), None), ((\"Atlanta's\", 'NOUN'), None), (('recent', 'ADJ'), None), (('primary', 'NOUN'), None), (('election', 'NOUN'), None), (('produced', 'VERB'), None), (('``', '.'), None), (('no', 'DET'), None), (('evidence', 'NOUN'), None), ((\"''\", '.'), None), (('that', 'ADP'), None), (('any', 'DET'), None), (('irregularities', 'NOUN'), None), (('took', 'VERB'), None), (('place', 'NOUN'), None), (('.', '.'), None)]\n",
      "\n",
      "La precisión del etiquetador affix en el corpus es:\n",
      "0.24\n"
     ]
    }
   ],
   "source": [
    "# Tarea 4\n",
    "# Importante del conjunto de etiquetas\n",
    "# Re-ejecutar las tareas 2 y 3 usando el etiquetado universal del corpus\n",
    "# Comparar las diferencias entre el etiquetado \"universal\" y el etiquetado completo\n",
    "news_sents = brown.tagged_sents(categories='news', tagset=\"universal\")\n",
    "science_fiction_sents = brown.tagged_sents(categories='science_fiction', tagset='universal')\n",
    "\n",
    "# Entrena el etiquetador affix\n",
    "affix_tagger = AffixTagger(news_sents)\n",
    "\n",
    "# Etiqueta el corpus\n",
    "affix_sents = affix_tagger.tag_sents(news_sents)\n",
    "\n",
    "# Imprime la primera frase y la precisión\n",
    "print(\"\\nLa primera frase etiquetada con el etiquetador affix es:\")\n",
    "print(affix_sents[0])\n",
    "print(\"\\nLa precisión del etiquetador affix en el corpus es:\")\n",
    "print(round(affix_tagger.evaluate(science_fiction_sents),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
